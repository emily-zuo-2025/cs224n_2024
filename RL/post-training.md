# LLM Post-Training

## 导言：超越预训练——后训练的必要性与核心思想

大语言模型（Large Language Models, LLM）的训练过程可清晰地划分为两个核心阶段：预训练（Pre-training）与后训练（Post-training）。预训练阶段通过在海量、无标注的文本数据上执行下一词元预测（Next-Token Prediction）任务，为模型奠定广泛的语言学基础。然而，仅仅依赖这种自监督学习范式，并不足以构建出可靠、可信、且能与真实人类意图精确对齐的智能系统。

预训练模型的固有局限性根植于其核心优化目标——最大似然估计（Maximum Likelihood Estimation）。该目标使模型学习在给定上下文中，生成统计上最可能的词元序列，本质上是对静态数据分布的拟合。

这导致了三大核心问题：首先，模型可能为了追求局部概率性而编造与事实不符的内容，即"幻觉"（Hallucinations）；其次，其生成过程缺乏明确的逻辑约束，难以在长篇幅推理中维持一致性；最关键的是，其价值取向由完全自由数据决定，可能与人类社会的伦理规范和价值观存在偏离。

因此，后训练阶段应运而生。其核心思想在于通过一系列精细化的调优与对齐技术，弥补预训练的不足。本文旨在系统性地拆析后训练阶段的三大技术支柱：**监督微调（Supervised Finetuning, FT）**、**强化学习（Reinforcement Learning, RL）** 和 **测试时扩展（Test-Time Scaling, TTS）**。我们将深入探讨它们各自的原理、核心算法以及它们如何协同作用，最终将一个具备基础语言能力的预训练模型，塑造为一个能够进行复杂推理、遵循用户意图、并符合伦理要求的可靠AI助手。

---

## 1. 背景知识：从序列预测到序贯决策的范式演进

要深刻理解大语言模型的后训练技术，必须首先认识其底层数学模型的一次关键范式演进：从静态的概率分布拟合，转向动态的序贯决策制定。这一理论上的飞跃，从根本上回答了"为什么强化学习（RL）是优化语言模型的有效路径？"这一核心问题。本节将为你揭示这一转变的理论基石。

### 1.1 语言模型的基石：最大似然估计（MLE）

在预训练阶段，语言模型优化的核心目标是最大似然估计（Maximum Likelihood Estimation, MLE）。其目标是让模型在给定上下文的条件下，预测下一个词元的概率尽可能接近真实数据中的词元。该目标函数表示为最小化负对数似然：

$$L_{MLE} = -\sum_{t=1}^{T} \log P_\theta(y_t|y_{<t}, X)$$

其中，各符号定义如下：

- **X**：输入，例如用户提示（Prompt）或上下文。
- **Y = (y_1, ..., y_T)**：目标输出序列。
- **θ**：模型的参数。
- **P_θ(y_t | y_{<t}, X)**：模型在给定输入 X 和已生成的历史词元 y_{<t} 的条件下，预测下一个词元为 y_t 的概率。

从信息论角度看，这个目标等价于最小化模型预测分布与真实数据分布之间的交叉熵。这种逐词元（Token-wise）的优化方式能够有效地训练模型生成局部流畅的文本。然而，它存在一个被称为"**暴露偏差**"的根本问题：模型在训练时始终以真实的、无误的历史词元为条件，但在推理时却需要依赖自身生成的、可能包含错误的词元。这种训练与推理之间的差异，可能导致错误在生成过程中不断累积，形成"级联错误"。

### 1.2 理论飞跃：将文本生成建模为马尔可夫决策过程（MDP）

为了克服MLE的局限性，研究者们将目光回归的文本生成过程抽象为一个序贯决策问题（Sequential Decision-Making Problem），并利用马尔可夫决策过程（Markov Decision Process, MDP）的框架进行建模。其理论依据在于，每生成一个词元，都可以看作是在当前状态下做出的一个特定状态下触发出的一个决策（动作），该决策将影响后续状态和最终的输出质量。

文本生成过程与MDP核心要素的映射关系如下表所示：

| MDP 要素 | 文本生成中的对应概念 |
|---------|-------------------|
| 状态（State, s_t） | 模型已生成的词元序列 (y_1, ..., y_{t-1})。 |
| 动作（Action, a_t） | 模型在当前状态下，从词汇表中选择并生成的下一个词元 y_t。 |
| 奖励（Reward, R(s_t, a_t)） | 一个评估函数，用于衡量生成序列的质量。这个类别可以是最终奖励（与人类偏好的一致性、事实的准确性等。 |

在此框架下，语言模型的策略 π_θ 被优化以最大化长期期望累积奖励，其总目标函数 J(π_θ) 定义为：

$$J(\pi_\theta) = E\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]$$

其中，折扣因子 γ 用于权衡即时奖励与未来奖励的重要性。γ 值越高，模型越注重长期的整体回报。

**对比分析**：MLE与RL的目标函数揭示了根本性的差异。MLE优化的是基于静态数据集的条件概率，**关注的是"生成像人类的文本"**；而RL则通过与环境（或奖励模型）的动态交互，优化一个衡量"**好坏**"的长期累积奖励，**关注的是"生成人类偏好的文本"**。

### 1.3 强化学习中的核心概念：价值与优势函数

为了有效地进行策略优化，强化学习引入了几个核心概念来评估决策的优劣：

- **价值函数（Value Function, V(s)）**：
  - $V(s) = E[\text{未来累积回报} \mid \text{当前状态 } s]$
  - 它衡量的是处于某个状态 s 的长期价值期望。换言之，从这个状态出发，遵循当前策略，平均能获得多少总回报。

- **动作价值函数（Q-Function, Q(s, a)）**：
  - $Q(s, a) = E[\text{未来累积回报} \mid \text{当前状态 } s, \text{执行动作 } a]$
  - 它衡量在状态 s 下执行特定动作 a 后，遵循当前策略所能获得的长期价值期望。

- **优势函数（Advantage Function, A(s, a)）**：
  - $A(s, a) = Q(s, a) - V(s)$
  - 优势函数通过减去状态的基线价值 V(s)，精确地分离出特定动作 a 的边际贡献。它回答了"在状态 s 下，采取这个特定动作比选择典型的平均表现更好多少？"这一个正确的优势函数极为关键：如果一个动作 a 优于平均选择，应当被鼓励；反之则应被抑制。此函数是进行精细分配的关键数学工具，我们将在后文看到，它被用来解决诸如结果导向奖励建模中难以定位具体错误步骤的问题。

从MLE到MDP的范式转变为后续所有基于强化学习的后训练方法奠定了坚实的理论基础。使得我们能够运用一系列强大的策略优化算法来精细地调整模型行为。

---

## 2. 强化学习（RL）优化：对齐模型与人类偏好

强化学习是实现大语言模型与真实、主观的人类偏好对齐的核心技术路径。它提供了一套系统性的方法论，将抽象的人类价值观转化为可计算的信号，并以此为导向，持续优化模型的生成策略。本节将深入探讨RL在LLM后训练中的核心范式：如何量化人类偏好（奖励建模），以及如何利用该信号指导模型策略的演进（策略优化）。

### 2.1 奖励建模（Reward Modeling, RM）：量化人类偏好

奖励建模的根本目标是训练一个参数化的模型 R_θ(x, y)，使其能够为给定的输入-输出对 (x, y)（即提示和模型的回答）打出一个标量分数。这个分数必须能够准确地反映人类对不回答的偏好排序，从而为后续的RL策略优化提供一个清晰、可计算的奖励信号。

为了训练奖励模型，通常需要收集人类偏好数据，并采用以下概率模型进行建模：

- **布拉德利-特里模型（Bradley-Terry Model）**：该模型适用于成对的偏好数据。直接给一个回答打具体的"分数"很难（比如 85 分 vs 86 分），判断"A 比 B 好"对人类很容易。例如，对于同一个提示 x，给定两个回答 y_j 和 y_k，如果人类标注者认为 y_j 优于 y_k（记为 y_j > y_k），模型通过最大化观测到这一偏好对的概率来学习。其损失函数如下：

$$\mathcal{L}_{BT}(\theta) = -\sum_{(x,y) \succ y_k} \log P(y_j \succ y_k|x; \theta)$$

其中，$P(y_j \succ y_k)$ 通常用 $\sigma(R_\theta(x, y_j) - R_\theta(x, y_k))$ 建模，σ 是Sigmoid函数。损失函数的目标是让偏好回答的奖励分数量著高于非偏好回答。

$$P(y_j \succ y_k) = \sigma(R_\theta(x, y_j) - R_\theta(x, y_k)) = \frac{1}{1 + e^{-(R_1 - R_2)}}$$

![Sigmoid函数图像](sigmoid_curve)

- **BT模型的优势：**
  - **消除绝对分数的噪音**：不同的人打分标准不同，但"A比B好"这种相对关系通常是一致的。Bradley-Terry 模型专门捕捉这种相对偏好。
  - **梯度优化**：Sigmoid 函数光滑可导，非常适合用于神经网络的梯度下降训练。

- **普拉克特-卢斯模型（Plackett-Luce Model）**：该模型适用于更复杂的场景，即人类对多个回答进行了完整的排序。它通过将一个不完整的排序概率分解为一系列条件选择概率的乘积来进行优化。

排序 A>B>C 的总概率定义为：

$$P(A \succ B \succ C) = P(\text{A is top} \mid A, B, C) \times P(\text{B is top} \mid B, C)$$

其损失函数如下：

$$\mathcal{L}_{PL}(\theta) = -\sum_{(x,\text{rank}) \in D} \sum_{\ell=1}^{m} \log\left(\frac{\exp(R_\theta(x, y_{j_\ell}))}{\sum_{k=\ell}^{m} \exp(R_\theta(x, y_{j_k}))}\right)$$

其中，外层求和遍历所有排序数据，内层求和则遍历排序中的每一个位置。

在实践中，奖励信号的设计可以分为两种主要范式：**结果导向和过程导向**。

| 特征 | 结果导向奖励（Outcome Reward Modeling, ORM） | 过程导向奖励（Process Reward Modeling, PRM） |
|------|-------------------------------------------|-------------------------------------------|
| 评估对象 | 仅评估最终生成的完整答案。 | 评估生成过程中的每一个中间推理步骤。 |
| 优点 | 实现简单，标注成本相对较低。 | 提供更细粒度的反馈，利于解决大信用分配问题，激励渐进性的逻辑链。 |
| 缺点 | 信用分配困难；难以确定长序列中是哪个具体的词元（动作）导致了最终的低质量结果。 | 标注过程复杂，需要"黄金"推理路径或专家逐步推理评分。 |
| 适用场景 | 简短问答、客观正确性明确的任务。 | 数学推导、代码生成、法律论证等多步推理任务。 |

研究表明，过程导向奖励（PRM）如果结合对最后一步结果的评估，其性能通常优于单纯的结果导向奖励（ORM），因为它能为策略更新提供更精确和有效的指导。

### 2.2 策略优化（Policy Optimization）：基于奖励信号进行模型迭代

获得奖励模型后，下一步是利用其提供的奖励信号来优化LLM的生成策略。以下是几种主流的策略优化方法。

#### 2.2.1 近端策略优化（Proximal Policy Optimization, PPO）

- **核心思想**：PPO通过引入一个"截断（clipping）"机制，来限制每次策略更新的步幅，从而在积极探索新策略和维持训练稳定性之间取得精妙平衡，确保更新后的策略不会与旧策略偏离太远。

- **目标函数**：

$$\mathcal{L}^{PPO}(\theta) = E_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

其中 $r_t(\theta)$ 是概率比率 $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$，$\hat{A}_t$ 是优势函数估计。

此外，奖励模型是不完美的，为了防止模型知道金奖励模型而"玩火入魔"（即奖励 hacking），导致其基础语言能力退化，PPO会加入KL散度惩罚项：$\beta * D_{KL}(\pi(\cdot|x)||\pi_{ref}(\cdot|x))$。该项是关于重要的正则化路，其中 ρ 是冻结的SFT模型，作为语言质量的"基准"。此惩罚项防止策略 π 为最大化奖励而生成荒谬错误或无意义的文本。

**PPO算法流程：**

**A. 训练阶段：**

1. **输入（Input）**：
   - 提示数据集 $D_{prompt}$：一批提示 {x}
   - **格式**：Token ID序列
   - **值域**：[0, vocab_size-1]
   - **学习策略模型 π_θ**：初始为SFT模型副本，参数 θ 可训练。
   - **参考策略模型 π_ref**：冻结的SFT模型，参数 θ_ref 不变
   - **奖励模型 R_ψ**：预训练好的奖励模型，参数 ψ 冻结。
   - **价值函数模型 V_ϕ**：Critic 网络，参数 ϕ 可训练。

2. **核心运算（Core Operation）**：
   - **采样（Sampling）**：对每个提示 x，使用当前策略 π_θ 生成一个响应 y，记录每个时间步的 (s_t, a_t)，对数概率和价值 V_ϕ(s_t)
   
   - **奖励计算（Reward Computation）**：对完整的 (x, y)，使用 R_ψ 计算奖励奖励 r。结合KL惩罚项，得到最终奖励 $r' = r - \beta * D_{KL}(\pi_\theta(\cdot|x)||\pi_{ref}(\cdot|x))$
   
   - **优势估计（Advantage Estimation）**：使用采样轨迹中的奖励 r' 和价值 V_ϕ，通过广义优势估计（GAE）计算每个时间步的优势 $\hat{A}_t^{GAE} = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + ...$，
     
     其中 $\delta_t = r_t + \gamma V(s_t + 1) - V(s_t)$ 为单步优势
     
     - **λ=0, 退化为 One-step TD:**
       - 方差小：非常稳定，因为V是训练获平滑过的
       - 偏差大：如果 Critic 本身很烂（训练初期通常），这个计算结果就是错的
     
     - **λ=1, 退化为Monte Carlo,**
       - 偏差小：用的是真实发生的奖励
       - 方差大：后续生成的文本具有随机性，偶尔一次写得好可能是运气。用这种不稳定的数据去更新模型，训练会大幅震荡
     
     - 通常将 λ 设为 0.95 左右，意味着既利用了真实的长期奖励序列（降低偏差），又利用了价值函数的预测来平滑数据（降低方差）
   
   - **梯度更新（Gradient Update）**：
     - 使用PPO的截断目标函数，根据优势 $\hat{A}_t$ 和概率比率 $r_t$，更新策略模型 π_θ 的参数 θ
     - 使用价值函数损失（如MSE），更新价值模型 V_ϕ 的参数 ϕ。
     
     两个更新交替（或同时）进行，使得模型在"想赢（高奖励）"和"懂行（准确评估）"这两个维度上螺旋上升。

3. **输出（Output）**：
   - 更新后的模型参数 θ' 和 ϕ'。这些参数将作为下一轮迭代的新输入。

**B. 推理阶段：**

a. **输入（Input）**：用户提示 x（Token ID 序列）。

b. **核心运算（Core Operation）**：使用最终优化好的策略模型 π_θ' 进行一次前向传播，自回归地生成响应 y。

c. **输出（Output）**：生成的文本响应 y。

#### 2.2.2 直接偏好优化（Direct Preference Optimization, DPO）

- **核心思想**：DPO巧妙地将奖励函数隐式地嵌入策略本身来表示，从而将复杂的RL优化问题转化为一个简单的、基于偏好对的分类损失函数，完全绕过了显式的奖励建模和复杂的RL训练循环。

- **目标函数**：

$$\mathcal{L}^{DPO}(\theta) = -E_{(x,y^+,y^-) \sim D}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y^+|x)}{\pi_{ref}(y^+|x)} - \beta \log \frac{\pi_\theta(y^-|x)}{\pi_{ref}(y^-|x)}\right)\right]$$

其中 $(x, y^{\wedge+}, y^{\wedge-})$ 是一个偏好对，π_ref 是参考策略。该损失函数的目标是最大化策略 π_θ 对偏好回答 y+ 相对于参考策略的对数概率增益，同时最小化对非偏好回答 y- 的增益。

先计算两个对数概率差增益的差值，然后用Sigmoid 将这个差值映射到 0-1 之间，差值越大，Sigmoid 越接近 1，log(1)=0，损失最小。

为确保训练稳定，DPO数据集通常会经过因感度过滤：在参考模型 π_ref 下困惑度过高的响应会被移除，以确保训练数据与初始SFT策略的分布保持一致。

DPO 是离线（Off-Policy）算法，这意味着模型只使用已有的数据集进行训练，不需要在训练过程中实时生成新数据。

**训练阶段算法流程：**

1. **输入（Input）**：
   - **偏好数据集 D**：一批三元组 $\{(x, y+, y-)\}$。
     - **格式**：Token ID序列的元组
     - **形状**：单个序列为 [seq_len]，**值域**：[0, vocab_size-1]
   - **参考策略模型 π_ref**：冻结的SFT模型，参数 θ_ref 不变。
   - **学习策略模型 π_θ**：初始为SFT模型副本，参数 θ 可训练。

2. **核心运算（Core Operation）**：
   - **前向传播**：对于每个偏好对 $(x, y+, y-)$，分别计算 π_θ 和 π_ref 在给定 x 的条件下生成 y+ 和 y- 的对数概率。
   
   $$\log \pi(y|x) = \sum_{t=1}^{|y|} \log \pi(y_t|x, y_{<t})$$
   
   这个计算只需要模型进行一次高效的前向传播，无需像 PPO 那样进行耗时的自回归采样。
   
   - **损失计算**：利用上一步计算的四个对数概率，代入 DPO 损失函数：
     - **计算偏好优势**：$\text{Advantage} = \beta(\log r_\theta(y^+) - \log r_\theta(y^-))$
     
       希望这个优势（即 $y^+$ 的增益相比 $y^-$ 的增益）是大的正数
     
     - **通过 Sigmoid 映射**：$\sigma(\text{Advantage})$
       
       这将其转化为一个接近 1 的概率（代表 πθ 成功识别 $y^+ \succ y^-$ 的概率）
     
     - **负对数似然**：$-\log \sigma(\text{Advantage})$
       
       将其转化为我们希望最小化的损失。如果概率接近 1，损失接近 0；如果概率接近 0，损失很高。
   
   - **梯度更新**：对策略模型 π_θ 的参数 θ 进行梯度下降更新：
     
     $$\theta \leftarrow \theta - \eta \cdot \nabla_\theta \mathcal{L}^{DPO}$$

3. **输出（Output）**：
   - 更新后的策略模型参数 θ'，作为下一轮迭代的输入。

#### 2.2.3 组相对策略优化（Group Relative Policy Optimization, GRPO）

- **核心思想**：GRPO是PPO的一种高效变体，其关键创新在于它省去了独立的价值函数网络（Critic）。它通过在针对同一提示生成的多个响应组内计算平均奖励，并以此作为基线（baseline）来估计每个响应的优势函数，极大地降低了训练的内存消耗和实现复杂性。

- **目标函数**：GRPO的目标函数形式上与PPO类似，但其优势函数 $\hat{A}_{i,t}$ 的计算方式不同。对于一个包含 G 个响应的组，第 i 个响应的优势被设定为其归一化奖励 $r\_i$：$r\_i = (r_i - \text{mean}(r)) / \text{std}(r)$ 这个归一化奖励直接反映了该响应在组内的相对优势。

**训练阶段算法流程：**

1. **输入（Input）**：
   - **提示数据集 D_prompt**：一批提示 {x}（Token ID 序列）。
   - **学习策略模型 π_θ**：初始为SFT模型副本，参数 θ 可训练
   - **参考策略模型 π_ref**：冻结的SFT模型。
   - **奖励模型 R_ψ**：预训练好的奖励模型。
   - **组大小 G**：整数超参数。

2. **核心运算**：
   - **组采样**：模型 $\pi_{\theta,i}$ 针对每一个提示 x，随机采样生成 G 个不同的回答 $\{o_1, o_2, ..., o_G\}$。
   - **奖励计算**：使用 $R_\psi$ 为每个响应 $o_i$ 打分，得到奖励 $\{r_1, ..., r_G\}$。
   - **优势估计（Advantage Estimation via Group Normalization）**：
     
     i. 先计算这组分数的均值和标准差：
        
        $$\text{mean} = \frac{1}{G}\sum r_i, \quad \text{std} = \sqrt{\frac{1}{G}\sum(r_i - \text{mean})^2}$$
     
     ii. 然后计算每个回答的相对优势：$\hat{A}_i = \frac{r_i - \text{mean}}{\text{std} + \epsilon}$
         
         理解：Z-Score 标准化，不管这个问题本身的难度，只在平在组内的相对排名
   
   - **梯度更新（Gradient Update）**：使用类似于PPO的截断目标函数和KL惩罚，对策略模型 π_θ 进行更新：
     
     $$\mathcal{L}_{GRPO}(\theta) = -\frac{1}{G}\sum_{i=1}^{G}\left[\min\left(\frac{\pi_\theta(o_i|x)}{\pi_{\theta_{old}}(o_i|x)}\hat{A}_i, \text{clip}(...)\hat{A}_i\right) - \beta D_{KL}\right]$$

3. **输出（Output）**：
   - 更新后的策略模型参数 θ'。

#### 2.2.4 核心算法对比

| 维度 | 近端策略优化（PPO） | 直接偏好优化（DPO） | 组相对策略优化（GRPO） |
|------|-------------------|-------------------|---------------------|
| 策略学习机制 | 在线策略梯度方法，通过截断概率比和KL散度惩罚进行增量式更新。 | 将RL问题转化为基于偏好对的监督学习问题，直接优化分类损失。 | PPO的变体，同样使用概率比和截断目标。 |
| 参考模型作用 | 作为KL散度惩罚的基准，防止策略过度偏离初始策略"玩火入魔"，保持语言能力。 | 作为计算对数概率比的固定基础，是损失函数中完全绕过了价值网络。 | 与PPO作用相同，作为KL惩罚的基准。 |
| 奖励/优势计算 | 需要一个独立的奖励模型和一个价值函数网络（Critic）来估计优势 A(s, a)。 | 无显式奖励/优势计算。奖励被隐式地编码在偏好对和策略的对数概率中，完全绕过了价值网络。 | 仅需一个奖励模型。优势通过对同一样本组内的奖励进行归一化估计，无需价值网络。 |

#### 2.2.5 其他重要优化算法

- **信任区域策略优化（Trust Region Policy Optimization, TRPO）**：作为PPO的前身，TRPO通过一个硬性的KL散度约束来保证每次策略更新都在一个可信的"信任区域"内，确保了更新的稳定性。然而，它依赖于复杂的二阶优化方法（如共轭梯度法），计算成本较高，因此在LLM等大规模应用中不如PPO流行。

- **离线推理优化（Offline Reasoning Optimization, OREO）**：OREO是一种离线强化学习方法，它通过优化模型在系统演练数据集上的表现来提升模型的推理能力。该方法充分利用现有的、不需要在训练过程中实时生成新数据的离线数据集，这种方法让模型在大规模固定数据集上进行更细粒度的优化。这步骤的信用分配，与DPO基于个轨迹的偏好相比，更精细地捕捉到位置相关性，更精确地将好坏归定位错误根源。

- **赔率比偏好优化（Odds Ratio Preference Optimization, ORPO）**：ORPO是DPO的一种简化形式，它巧妙地将偏好响应与非偏好响应的比值（odds ratio）直接嵌入损失函数。其核心思路是通过将好回答与差回答的对数概率比，同时增加偏好回答的生成概率（类似SFT）并减弱偏好约束，从而简化了对齐过程。

### 2.3 纯强化学习驱动的优化流程：DeepSeek-R1案例分析

以DeepSeek-R1模型为例，我们可以看到一个不依赖大规模初始SFT，而以RL为核心的多阶段后训练流程。这种流程展示了RL在模型能力塑造中的主导作用。

1. **冷启动RL阶段**：首先，使用少量精心策划的数据进行初步的RL调通。这个阶段的主要作用是为模型注入初步的推理结构和格式遵循能力，为后续更复杂的RL训练提供一个稳定的起点。

2. **拒绝采样与微调**：在模型经过初步RL稳定后，利用它生成大量候选响应。通过一个验证器（例如，检查数学答案的正确性）进行"拒绝采样"，筛选出高质量的样本。这些高质量样本随后被用于进行一次SFT，从而增大其质量空间的覆盖面。

3. **面向推理的RL**：在此阶段，使用如GRPO这样的高效RL算法，并结合基于规则的奖励函数（例如，数学解题的正确性、代码执行的通过率），来专门强化模型的复杂推理能力。

4. **二次RL对齐**：接着进行第二轮RL，这次的目标更广泛地对齐人类偏好，例如回答的有用性（helpfulness）、无害性（harmlessness）和创造性。

5. **模型蒸馏（Distillation）**：最后，将这个经过多阶段RL优化后的大模型（教师模型）的能力，通过知识蒸馏的方式迁移到一个或多个更小的模型（学生模型）中，从而在保持高性能的同时，实现更高效的部署。

总而言之，强化学习通过精细的奖励设计和先进的策略优化算法，已成为提升大语言模型推理能力和对齐水平最强大、最灵活的工具。然而，所有RL对齐的成功，都离不开一个高质量的初始策略，这正是监督微调（SFT）所扮演的关键角色。

---

## 3. 监督微调（SFT）：模型能力专业化的基础

监督微调（Supervised Finetuning, SFT）是整个后训练流程中的基石。它扮演着引导预训练模型从一个通用的语言理解器，转变为能够适应特定任务格式、学握特定领域知识并遵循特定交互风格的"专业模型"的第一步。一个高质量的SFT阶段，不仅能直接提升模型在目标任务上的性能，更能为后续更复杂的强化学习对齐阶段提供一个优秀的初始策略模型。

### 3.1 SFT的核心方法论

SFT通过在有标签的"指令-回答"对数据集上进行训练，教会模型如何根据用户的输入生成期望的输出。根据目标的不同，SFT可以分为以下几种类型：

- **指令微调**：这是最常见的SFT形式，其目标是训练模型遵循用户指令的能力。通过在覆盖了成千上万种任务类型的数据上（如问答、翻译、摘要、创作等）进行微调，模型学会了文化则和识别用户意图。Alpaca 是一个典型的指令微调能力的典型例子。

- **对话微调**：为了让模型能进行流畅、有上下文感知能力的多轮对话，需要使用对话格式的数据进行微调。这些数据通常包含多轮用户提问和模型回答的聊天记录。通过这种方式训练的模型，如ChatGPT，能够更好地把握对话上下文，提供更具互动性的体验。

- **思维链（CoT）推理微调**：为了提升模型的复杂推理能力，可以专门使用包含"思维链"的数据进行微调。在这种数据中，模型的输出不仅包含最终答案，还包含得出答案的详细、逐步的推理过程。这不仅能提升模型在数学、逻辑等任务上的准确率，也增强了其输出的可解释性。

- **领域专属微调**：当需要模型在特定专业领域（如生物医学、金融、法律）表现出色时，就需要使用该领域的专业文本和问答数据进行微调。例如，**BioGPT** 在生物医学文献上进行微调，**FinBERT** 专注于金融文本，这使得它们能更好地理解和生成特定领域的语言和知识。

- **基于蒸馏的微调**：在这种模式下，一个更强大的"教师"模型（如GPT-4）被用来生成大量高质量的指令-回答对数据。然后，一个更小的"学生"模型在这些生成的数据上进行SFT。这种方法可以有效地将大模型的能力迁移到小模型上，实现性能与效率的平衡。

### 3.2 参数高效微调（Parameter-Efficient Finetuning, PEFT）

对动辄数百亿参数的大语言模型进行全量微调（Full Finetuning）会带来巨大的计算和存储开销，这在很多场景下是不现实的。参数高效微调（PEFT）技术的出现正是为了解决这一挑战。

**PEFT的核心思想是**：冻结预训练模型的大部分（通常是全部）参数，仅引入并训练少量额外的、可学习的参数。

**PEFT 的核心假设**："**内在维度假说**" —— 模型虽然有数百亿参数，但完成特定下游任务时，真正起作用的"有效参数空间"其实非常小（低秩的）。我们不需要改动所有参数，只要找到那个小的关键空间改动一点点就够了。

**全量微调的痛点：**

1. **显存爆炸**：微调一个模型所需的显存不仅仅是加载模型权重。还需要存储梯度、优化器状态（Optimizer States, 如 AdamW 需要存两倍）

2. **灾难性遗忘**：为了学会一个新任务（比如写代码），全量修改参数可能会导致模型"忘掉"旧知识（比如原本很好的对话能力或通识知识）

**代表技术：LoRA（Low-Rank Adaptation）**

#### A. 核心原理：旁路更新

LoRA的原理建立在模型的Transformer层中注入两个小矩阵的低秩矩阵。在微调过程中，只有这两个小矩阵的参数被更新，而庞大的原始权重矩阵保持不变。由于这两个矩阵的参数量远小于原始权重，LoRA能够以极低的成本实现与全量微调相近的性能。即预训练模型的权重 $W_0$ 不动，增量 $\Delta W$ 用来学习权重：$W_{final} = W_0 + \Delta W$

#### B. 低秩分解（Low-Rank Decomposition）

直接训练 $\Delta W$ 参数量依然很大（因为它和 $W_0$ 形状一样）。LoRA 将 $\Delta W$ 分解为两个极小的矩阵 A 和 B 相乘：

$$\Delta W = B \times A$$

- 假设 $W_0$ 是 $d \times d$ 的大矩阵（比如d=4096）。
- **矩阵 A**：形状为 $r \times d$（降维）。
- **矩阵 B**：形状为 $d \times r$（升维）。
- **秩（Rank, r）**：这是一个超参数，通常设得很小（如 8, 16, 64）。

**参数量对比：**

- **全量**：需要训练 $4096 \times 4096 \approx 1600$ 万个参数。
- **LoRA (r=8)**：只需要训练 $(4096 \times 8) + (8 \times 4096) \approx 6.5$ 万个参数。
- **结论**：参数量减少了 **1000 倍以上**，显存占用大幅下降。

#### C. 零推理延迟（Zero Inference Latency）

这是 LoRA 相比其他 PEFT 方法（如 Adapter）最大的优势。

在推理阶段，我们可以利用矩阵乘法的分配律，直接把训练好的 $B \times A$ 加回到原始权重 $W_0$ 中：

$$W' = W_0 + BA$$

导出模型时，这就变成了一个普通的、更新过权重的模型。推理时不需要额外的计算步骤，速度与原模型完全一致。

**QLoRA（Quantized LoRA）- LoRA 的进化版**

如果说 LoRA 解决了训练参数量的问题，**QLoRA** 则进一步解决了底层模型加载的显存问题。它是目前最主流的微调方式（比如在单卡 24G 显存上微调 30B 模型）。

**核心动作：**

a. **4-bit 量化**：将冻结的底层模型 $W_0$ 压缩到 **4-bit**（NF4 格式），显存占用直接砍到 1/4。

b. **LoRA 微调**：在 4-bit 的底座上，加入 FP16 或 BF16 精度的 LoRA 适配器进行训练。

c. **双重量化 & 分页优化器**：进一步压榨显存极限。

**其他 PEFT 技术流派对比**

除了 LoRA，PEFT 还有其他几种流派，虽然现在不如 LoRA 流行，但理解它们有助于建立完整视角：

| 技术流派 | 代表方法 | 原理简述 | 缺点 |
|---------|---------|---------|------|
| 加性方法（Additive） | Adapter | 在 Transformer 层之间插入小型的神经网络层（Adapter Layer）。 | 增加推理延迟，因为是串行插入的，推理时必须经过这些层，无法像 LoRA 那样合并权重。 |
| 选择性方法（Selective） | BitFit | 不训练参数，只训练原模型的 Bias（偏置项），冻结其他。 | 效果通常不如 LoRA，尤其在复杂任务上。 |
| 重参数化（Reparametrization） | LoRA | 学习权重的低秩增量矩阵。 | 训练时显存占用高于 Adapter（需存 A, B 的激活值），但推理无损。 |
| Prompt Tuning | P-Tuning / Prefix Tuning | 不动模型内部，只在输入的 Embedding 层前面加一些可学习的"虚拟 Token"。 | 占用了宝贵的上下文窗口长度（Context Window）；在深层推理能力上往往不如 LoRA。 |

PEFT并非一项孤立技术，而是协同生态系统中的关键一环。PEFT（模型层）的全部潜力，是在与系统层和数据层优化相结合时才得以释放。例如，PEFT技术与分布式训练框架（如DeepSpeed，系统层）和高效的数据过滤与压缩策略（数据层）协同工作，共同构建了一个全面的、高性价比的LLM适配与部署方案。

SFT为模型提供了实实在在的任务基础和行为先验，也它本质上是一种静态的学习过程，难以应对推理过程中动态变化的复杂情况。要实现更灵活和细致的对齐，则需要借助测试时推理策略的辅助。

---

## 4. 测试时扩展（TTS）：推理阶段的动态推理增强

测试时扩展（Test-Time Scaling, TTS）是一系列在不更新模型权重的前提下，通过在推理阶段投入更多计算资源来提升模型性能（尤其是复杂推理能力）的策略集合。TTS的核心思想是"用计算换精度"，它允许模型在面对复杂问题时进行更深度的"思考"、"探索"和"验证"，从而超越单次前向传播所能达到的性能上限。

### 4.1 基于搜索的推理策略

这类策略通过系统性地探索多种可能的生成路径，来寻找最优的答案。

1. **集束搜索（Beam Search）**：这是一种经典的搜索算法。在文本生成的每一步，它不再只选择概率最高的单个词元（贪心策略），而是保留 k 个最可能的候选序列（即"集束"）。通过这种方式，Beam Search能够系统性地探索高概率的推理路径，避免因早期的一次次优选择而错失全局最优解。

2. **最佳N选搜索（Best-of-N Search）**：这种策略采用"生成-验证"范式。它首先独立地、随机地采样盐原 N 个完全独立的答案，然后利用一个外部的验证器（例如，训练好的奖励模型）对这 N 个候选进行评分，最终选出得分最高的一个作为输出。与Beam Search相比，Best-of-N 能够探索更多样化的答案空间，有时能发现一些概率不高但质量很好的解。

3. **蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）**：MCTS是一种更智能的搜索策略，尤其适用于需要多步规划过程大量的随机模拟（rollouts）来评估不同推理路径的长期价值，并巧妙地在"探索"（exploration, 尝试新的、未知的路径）和"利用"（exploitation, 深入挖掘已知的高价值路径）之间进行平衡，从而能够更高效地在巨大的搜索空间中定位最优解。

### 4.2 结构化推理与自我修正

这类策略通过引导模型生成结构化的思考过程或进行迭代式的自我纠错来提升推理质量。

1. **思维链（Chain-of-Thought, CoT）**：CoT通过简单的提示（例如，"让我们一步一步地思考"）来引导模型在给出最终答案之前，先输出中间的、详细的推理步骤。这种方式能够激发模型潜在的序贯推理能力，显著提升其在数学、逻辑和常识推理任务上的表现。

2. **自我一致性（Self-Consistency）**：这一策略建立在统计学原理之上。它首先通过采样生成多条不同的思维链推理路径，然后对这些路径得出的最终答案进行"多数投票"。其核心洞察是：对于一个有唯一正确答案的问题，不同但有效的推理路径最终应收敛到同一个答案。因此，出现频率最高的答案被认为是最可靠的。

3. **思维树（Tree-of-Thoughts, ToT）与 思维图（Graph-of-Thoughts, GoT）**：
   
   - **ToT** 将CoT的线性推理链扩展为了树状结构。这使得模型不再局限于单一的推理路径，而是可以同时探索多个分支，并允许进行前瞻（lookahead）回溯（backtracking），极大地增强了问题解决能力。
   
   - **GoT** 则通过引入更灵活的图结构，超越了ToT的树状限制。在GoT中，不同的推理节点（思想）可以被灵活地聚合（aggregation）（例如，将两条推理径的优点合并成一个更优的路径）或精炼（refinement）。这种图结构提高了计算效率和推理的灵活性，使其能够解决更复杂的问题。

4. **通过修正实现自我提升（Self-Improvement via Refinements）**：这类方法构建了一个"生成-批判-修正"的迭代循环。例如，**Self-Refine** 方法首先让模型生成一个初始答案，然后让模型扮演批评角的"批评家"，找出答案中的错误或不足之处，并基于这些批评对生成一个修正后的版本。这个过程可以重复多次，直到模型的输出质量达到满意为止。

### 4.3 计算最优扩展策略（Compute-Optimal Scaling, COS）

COS策略的核心洞察是：并非所有问题都需要同等强度的计算投入。简单问题用复杂的搜索策略是浪费，而难题用简单的策略则无法解决。

其工作流程如下：

1. **评估问题难度**：首先，模型或一个外部验证器会评估输入问题的难度。

2. **分配计算策略**：
   
   - 对于简单问题，分配序贯修正（sequential refinement）资源，即让模型进行自我迭代修正。
   
   - 对于难题，则分配并行采样（parallel sampling）搜索资源（如Best-of-N或Beam Search），以探索更多可能性。

通过这种动态的、与问题难度相匹配的资源分配方式，COS实现了计算效率和性能之间的最优平衡。原文报告称，该策略相较于传统的Best-of-N采样，能够以高达**4倍的计算效率提升**实现同等性能，展示了在推理效率上的显著优势。

测试时扩展（TTS）方法为增强LLM的推理能力提供了一个强大的、无需重新训练的工具箱。它与模型的预训练、监督微调以及强化学习优化形成了关键的互补关系，共同构成了提升LLM综合能力的完整图景。