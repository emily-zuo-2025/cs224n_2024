\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}

\begin{document}

\section*{Proof that Naive-Softmax Loss equals Cross-Entropy Loss}

\textbf{Given:}
\begin{itemize}
    \item $\mathbf{y}$: true distribution (one-hot vector)
    \item $\hat{\mathbf{y}}$: predicted distribution (softmax output)
    \item $\hat{y}_o$: the predicted probability for the true class $o$
\end{itemize}

\textbf{Naive-softmax loss (Equation 2)} is typically defined as:
\begin{equation*}
J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U}) = -\log P(O=o|C=c) = -\log(\hat{y}_o) = -\log\left(\frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{w=1}^W \exp(\mathbf{u}_w^\top \mathbf{v}_c)}\right)
\end{equation*}

\textbf{Cross-entropy loss} between $\mathbf{y}$ and $\hat{\mathbf{y}}$ is defined as:
\begin{equation*}
CE(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{w=1}^W y_w \log(\hat{y}_w)
\end{equation*}

\textbf{Proof:}

Since $\mathbf{y}$ is a one-hot vector where the true class is at position $o$:
\begin{itemize}
    \item $y_o = 1$ (for the true class)
    \item $y_w = 0$ for all $w \neq o$
\end{itemize}

Substituting into the cross-entropy formula:
\begin{equation*}
CE(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{w=1}^W y_w \log(\hat{y}_w) = -y_o \log(\hat{y}_o) - \sum_{w \neq o} y_w \log(\hat{y}_w)
\end{equation*}

Since $y_w = 0$ for all $w \neq o$:
\begin{equation*}
CE(\mathbf{y}, \hat{\mathbf{y}}) = -1 \cdot \log(\hat{y}_o) - 0 = -\log(\hat{y}_o)
\end{equation*}

Therefore:
\begin{equation*}
J_{\text{naive-softmax}} = -\log(\hat{y}_o) = CE(\mathbf{y}, \hat{\mathbf{y}})
\end{equation*}

This completes the proof showing that the naive-softmax loss is equivalent to the cross-entropy loss when the true distribution is a one-hot vector.

\section*{(b) Gradient Analysis of Naive-Softmax Loss}

\subsection*{(b)(i) Partial Derivative with respect to $\mathbf{v}_c$}

\textbf{Answer:}
\begin{equation*}
\frac{\partial J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U})}{\partial \mathbf{v}_c} = \mathbf{U}^\top(\hat{\mathbf{y}} - \mathbf{y})
\end{equation*}

\textbf{Derivation:}

The naive softmax loss is:
\begin{equation*}
J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U}) = -\log P(O=o|C=c) = -\log\left(\frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{w \in \text{Vocab}} \exp(\mathbf{u}_w^\top \mathbf{v}_c)}\right)
\end{equation*}

This simplifies to:
\begin{equation*}
J = -\mathbf{u}_o^\top \mathbf{v}_c + \log\left(\sum_{w} \exp(\mathbf{u}_w^\top \mathbf{v}_c)\right)
\end{equation*}

Taking the partial derivative with respect to $\mathbf{v}_c$:
\begin{align*}
\frac{\partial J}{\partial \mathbf{v}_c} &= -\mathbf{u}_o + \frac{\sum_w \exp(\mathbf{u}_w^\top \mathbf{v}_c) \cdot \mathbf{u}_w}{\sum_w \exp(\mathbf{u}_w^\top \mathbf{v}_c)} \\
&= -\mathbf{u}_o + \sum_w \frac{\exp(\mathbf{u}_w^\top \mathbf{v}_c)}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)} \cdot \mathbf{u}_w \\
&= -\mathbf{u}_o + \sum_w \hat{y}_w \cdot \mathbf{u}_w
\end{align*}

In vectorized form, since $\mathbf{u}_o = \mathbf{U}^\top \mathbf{y}$ (where $\mathbf{y}$ is the one-hot vector):
\begin{equation*}
\frac{\partial J}{\partial \mathbf{v}_c} = \mathbf{U}^\top\hat{\mathbf{y}} - \mathbf{U}^\top \mathbf{y} = \mathbf{U}^\top(\hat{\mathbf{y}} - \mathbf{y})
\end{equation*}

\subsection*{(b)(ii) When is the Gradient Zero?}

\textbf{Answer:}

The gradient equals zero when:
\begin{equation*}
\hat{\mathbf{y}} = \mathbf{y}
\end{equation*}

This occurs when the predicted probability distribution equals the true distribution, i.e., when the model predicts the correct outside word with probability 1 and all other words with probability 0.

\subsection*{(b)(iii) Interpretation of the Gradient Terms}

The gradient $\mathbf{U}^\top(\hat{\mathbf{y}} - \mathbf{y})$ consists of two terms:

\begin{enumerate}
\item \textbf{$\mathbf{U}^\top\hat{\mathbf{y}}$ (positive term):} This is a weighted sum of all outside word vectors, weighted by their predicted probabilities. When subtracted from $\mathbf{v}_c$ during gradient descent, it \textbf{pushes the center word vector away} from the average position of all words that the model incorrectly believes are likely to be context words. This reduces false positive predictions.

\item \textbf{$-\mathbf{U}^\top \mathbf{y} = -\mathbf{u}_o$ (negative term):} This is the negative of the true outside/context word vector $\mathbf{u}_o$. When subtracted from $\mathbf{v}_c$ during gradient descent, the negative sign causes this term to \textbf{pull the center word vector toward} the actual context word $\mathbf{u}_o$, making them more similar. This increases the dot product $\mathbf{u}_o^\top \mathbf{v}_c$ and thus the probability of correctly predicting the true context word.
\end{enumerate}

Together, these updates improve $\mathbf{v}_c$ by making it more similar to actual context words and less similar to words that shouldn't be context words.

\section*{(c) L2 Normalization in Word Embeddings}

\subsection*{Question:}
In a binary sentiment classification task (positive/negative) where we sum word embeddings to classify phrases, when would L2 normalization (using $\mathbf{u}/||\mathbf{u}||_2$ instead of raw $\mathbf{u}$) take away useful information? When would it not?

\subsection*{Answer:}

\textbf{Key Mathematical Insight from the Hint:}

Consider the case where $\mathbf{u}_x = \alpha \mathbf{u}_y$ for some scalar $\alpha$ and words $x \neq y$.

When $\alpha > 0$ (same direction):
\begin{equation*}
\frac{\mathbf{u}_x}{||\mathbf{u}_x||_2} = \frac{\alpha \mathbf{u}_y}{||\alpha \mathbf{u}_y||_2} = \frac{\alpha \mathbf{u}_y}{|\alpha| \cdot ||\mathbf{u}_y||_2} = \frac{\alpha \mathbf{u}_y}{\alpha \cdot ||\mathbf{u}_y||_2} = \frac{\mathbf{u}_y}{||\mathbf{u}_y||_2}
\end{equation*}

Thus, when $\alpha > 0$, the normalized vectors are \textbf{identical}, collapsing words with different magnitudes but same direction into the same representation.

\textbf{When L2 normalization TAKES AWAY useful information:}

Normalization is detrimental when \textbf{magnitude encodes meaningful semantic differences}:

\begin{enumerate}
\item \textbf{Sentiment intensity encoded in magnitude:} Consider words with the same sentiment direction but different intensities:
\begin{itemize}
\item ``excellent'' $\rightarrow$ $\mathbf{u}_{\text{excellent}}$ (large positive magnitude)
\item ``good'' $\rightarrow$ $\mathbf{u}_{\text{good}} = 0.3 \cdot \mathbf{u}_{\text{excellent}}$ (same direction, smaller magnitude)
\end{itemize}

For the phrase ``This is excellent'', the raw sum would give a strong positive score. For ``This is good'', the raw sum gives a weaker positive score, appropriately reflecting the intensity difference.

After normalization, both contribute identically to the sum, so ``This is excellent'' and ``This is good'' would receive the same classification score, losing the nuanced intensity information.

\item \textbf{Word importance/weight:} If sentiment-bearing words (``excellent'', ``terrible'') have larger magnitudes than neutral words (``is'', ``the''), the raw embeddings naturally weight sentiment words more heavily in the sum. Normalization makes all words contribute equally, potentially diluting the sentiment signal with neutral words.

\item \textbf{Multiple occurrences matter differently:} In raw form, ``good good good'' would sum to a stronger positive signal than ``good''. With normalization, multiple identical words contribute the exact same amount as a single occurrence, losing frequency information that might indicate emphasis.
\end{enumerate}

\textbf{When L2 normalization DOES NOT take away useful information:}

Normalization is benign or beneficial when \textbf{only direction matters}:

\begin{enumerate}
\item \textbf{Magnitude is arbitrary or noise:} If magnitude differences arise from training artifacts (e.g., word frequency in corpus) rather than semantic meaning, normalization removes noise and focuses on the meaningful directional information. Different words with the same sentiment should contribute similarly.

\item \textbf{Opposite sentiments preserved:} When $\alpha < 0$, normalization preserves the opposition:
\begin{equation*}
\frac{\mathbf{u}_x}{||\mathbf{u}_x||_2} = \frac{\alpha \mathbf{u}_y}{|\alpha| \cdot ||\mathbf{u}_y||_2} = \frac{\alpha}{|\alpha|} \cdot \frac{\mathbf{u}_y}{||\mathbf{u}_y||_2} = -\frac{\mathbf{u}_y}{||\mathbf{u}_y||_2}
\end{equation*}

This means ``good'' and ``bad'' (if $\mathbf{u}_{\text{bad}} = -\alpha \mathbf{u}_{\text{good}}$ with $\alpha > 0$) will still point in opposite directions after normalization, preserving the essential positive vs. negative distinction needed for binary classification.

\item \textbf{Balanced phrase representation:} When we want each word to contribute equally (e.g., to prevent longer phrases from having artificially larger scores just due to more words), normalization ensures fair contribution from each word based purely on sentiment direction, not arbitrary magnitude.

\item \textbf{Words are linearly independent:} If different words have embeddings that are not scalar multiples of each other (i.e., $\mathbf{u}_x \neq \alpha \mathbf{u}_y$ for any $\alpha$), normalization doesn't cause the collision problem described above. The directional differences are preserved.
\end{enumerate}

\textbf{Summary:} L2 normalization removes useful information when magnitude encodes meaningful properties like sentiment intensity or word importance. It preserves essential information when only directional relationships matter for classification, particularly the positive vs. negative distinction ($\alpha < 0$ case)

\section*{(d) Partial Derivative with respect to U}

\subsection*{Question:}
Write down the partial derivative of $J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U})$ with respect to $\mathbf{U}$, broken down in terms of the column vectors $\frac{\partial J}{\partial \mathbf{u}_1}, \frac{\partial J}{\partial \mathbf{u}_2}, \ldots, \frac{\partial J}{\partial \mathbf{u}_{|\text{Vocab}|}}$ (do not further expand these terms).

\subsection*{Answer:}

The partial derivative of $J$ with respect to the matrix $\mathbf{U}$ is:

\begin{equation*}
\frac{\partial J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U})}{\partial \mathbf{U}} = \left[\frac{\partial J}{\partial \mathbf{u}_1} \;\; \frac{\partial J}{\partial \mathbf{u}_2} \;\; \cdots \;\; \frac{\partial J}{\partial \mathbf{u}_{|\text{Vocab}|}}\right]
\end{equation*}

This is a matrix of the same shape as $\mathbf{U}$, where each column is the partial derivative with respect to the corresponding column vector of $\mathbf{U}$.

\section*{(e) Partial Derivatives with respect to Individual Outside Word Vectors}

\subsection*{Question:}
Compute the partial derivatives of $J_{\text{naive-softmax}}(\mathbf{v}_c, o, \mathbf{U})$ with respect to each outside word vector $\mathbf{u}_w$. Consider two cases: when $w = o$ (the true outside word) and when $w \neq o$ (all other words).

\subsection*{Answer:}

\textbf{Case 1: When $w = o$ (the true outside word)}

\begin{equation*}
\frac{\partial J}{\partial \mathbf{u}_o} = (\hat{y}_o - 1) \mathbf{v}_c
\end{equation*}

\textbf{Case 2: When $w \neq o$ (all other words)}

\begin{equation*}
\frac{\partial J}{\partial \mathbf{u}_w} = \hat{y}_w \mathbf{v}_c
\end{equation*}

\textbf{Unified form:} Both cases can be written as:
\begin{equation*}
\frac{\partial J}{\partial \mathbf{u}_w} = (\hat{y}_w - y_w) \mathbf{v}_c
\end{equation*}

where $y_w = 1$ if $w = o$ and $y_w = 0$ if $w \neq o$.

\subsection*{Derivation:}

Recall that the naive softmax loss is:
\begin{equation*}
J = -\log P(O=o|C=c) = -\log\left(\frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)}\right)
\end{equation*}

This simplifies to:
\begin{equation*}
J = -\mathbf{u}_o^\top \mathbf{v}_c + \log\left(\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)\right)
\end{equation*}

\textbf{Case 1: $w = o$ (true outside word)}

Taking the derivative with respect to $\mathbf{u}_o$:
\begin{align*}
\frac{\partial J}{\partial \mathbf{u}_o} &= \frac{\partial}{\partial \mathbf{u}_o}\left[-\mathbf{u}_o^\top \mathbf{v}_c + \log\left(\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)\right)\right] \\
&= -\mathbf{v}_c + \frac{1}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)} \cdot \exp(\mathbf{u}_o^\top \mathbf{v}_c) \cdot \mathbf{v}_c \\
&= -\mathbf{v}_c + \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)} \cdot \mathbf{v}_c \\
&= -\mathbf{v}_c + \hat{y}_o \mathbf{v}_c \\
&= (\hat{y}_o - 1) \mathbf{v}_c
\end{align*}

Note that $y_o = 1$ (since $o$ is the true word), so this can also be written as $(\hat{y}_o - y_o) \mathbf{v}_c$.

\textbf{Case 2: $w \neq o$ (other words)}

For any word $w \neq o$, the first term $-\mathbf{u}_o^\top \mathbf{v}_c$ does not depend on $\mathbf{u}_w$, so its derivative is zero:
\begin{align*}
\frac{\partial J}{\partial \mathbf{u}_w} &= \frac{\partial}{\partial \mathbf{u}_w}\left[-\mathbf{u}_o^\top \mathbf{v}_c + \log\left(\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)\right)\right] \\
&= 0 + \frac{1}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)} \cdot \exp(\mathbf{u}_w^\top \mathbf{v}_c) \cdot \mathbf{v}_c \\
&= \frac{\exp(\mathbf{u}_w^\top \mathbf{v}_c)}{\sum_{w'} \exp(\mathbf{u}_{w'}^\top \mathbf{v}_c)} \cdot \mathbf{v}_c \\
&= \hat{y}_w \mathbf{v}_c
\end{align*}

Note that $y_w = 0$ (since $w$ is not the true word), so this can also be written as $(\hat{y}_w - y_w) \mathbf{v}_c = (\hat{y}_w - 0) \mathbf{v}_c = \hat{y}_w \mathbf{v}_c$.

\section*{2. Machine Learning \& Neural Networks}

\subsection*{(a) Adam Optimizer}

\subsubsection*{(a)(i) Momentum - Reducing Update Variance (2 points)}

\textbf{Question:} Briefly explain how using $\mathbf{m}$ stops the updates from varying as much and why this low variance may be helpful to learning.

\textbf{Answer:}

Momentum reduces update variance by accumulating a weighted average of past gradients rather than using only the current minibatch gradient. Since $\mathbf{m}_{t+1} = \beta_1 \mathbf{m}_t + (1-\beta_1) \nabla_{\theta_t} J_{\text{minibatch}}(\theta_t)$ with $\beta_1 \approx 0.9$, the momentum vector $\mathbf{m}$ smooths out noisy gradient estimates by incorporating historical gradient information. This dampening effect means that random fluctuations in individual minibatch gradients tend to cancel out, while consistent gradient directions accumulate and reinforce over time.

Lower variance updates are helpful because they provide a more stable and reliable estimate of the true gradient direction. This stability allows the optimizer to use larger learning rates without overshooting, helps the model navigate flat regions or narrow valleys in the loss landscape more efficiently, and reduces oscillations that can slow down or destabilize training. Additionally, momentum helps the optimizer build velocity in consistent directions, allowing it to accelerate through plateaus and escape shallow local minima more effectively.

\subsubsection*{(a)(ii) Adaptive Learning Rates - Dividing by $\sqrt{\mathbf{v}}$ (2 points)}

\textbf{Question:} Since Adam divides the update by $\sqrt{\mathbf{v}}$, which of the model parameters will get larger updates? Why might this help with learning?

\textbf{Answer:}

Parameters with \textbf{smaller gradient magnitudes} will receive larger updates. Since $\mathbf{v}_{t+1}$ tracks the exponentially weighted average of squared gradients ($\mathbf{v}_{t+1} = \beta_2 \mathbf{v}_t + (1-\beta_2)(\nabla_{\theta_t} J \odot \nabla_{\theta_t} J)$), parameters with consistently large gradients will have large $\mathbf{v}$ values. The update rule $\theta_{t+1} = \theta_t - \alpha \mathbf{m}_{t+1}/\sqrt{\mathbf{v}_{t+1}}$ divides by $\sqrt{\mathbf{v}}$, which means parameters with small gradients (small $\mathbf{v}$) get divided by a small number, resulting in larger relative updates, while parameters with large gradients get scaled down.

This adaptive learning rate mechanism is beneficial because it automatically balances the learning rates across different parameters. Parameters that receive consistently large gradients (which might correspond to frequently updated features or high-variance inputs) get their updates dampened to prevent overshooting, while parameters with small gradients (which might correspond to rarely activated features or infrequent patterns) get amplified updates to ensure they still learn effectively. This is particularly helpful in neural networks where different parameters can have vastly different gradient scalesâ€”for example, parameters in early layers versus late layers, or parameters associated with rare versus common features. By normalizing the update magnitudes, Adam ensures more uniform and stable learning across all parameters, leading to faster convergence and better generalization.

\subsection*{(b) Dropout Regularization}

Dropout is a regularization technique where, during training, units in the hidden layer $\mathbf{h}$ are randomly set to zero with probability $p_{\text{drop}}$, and then the result is multiplied by a constant $\gamma$:

\begin{equation*}
\mathbf{h}_{\text{drop}} = \gamma \mathbf{d} \odot \mathbf{h}
\end{equation*}

where $\mathbf{d} \in \{0,1\}^{D_h}$ is a mask vector (each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $1-p_{\text{drop}}$), and $\gamma$ is chosen such that:

\begin{equation*}
\mathbb{E}_{p_{\text{drop}}}[\mathbf{h}_{\text{drop}}]_i = h_i \quad \text{for all } i \in \{1, \ldots, D_h\}
\end{equation*}

\subsubsection*{(b)(i) Value of $\gamma$ in terms of $p_{\text{drop}}$ (2 points)}

\textbf{Answer:}

\begin{equation*}
\gamma = \frac{1}{1 - p_{\text{drop}}}
\end{equation*}

\textbf{Derivation:}

We need to find $\gamma$ such that the expected value of the dropped-out hidden layer equals the original hidden layer. Starting with the definition:

\begin{equation*}
\mathbb{E}[\mathbf{h}_{\text{drop}}]_i = \mathbb{E}[\gamma d_i h_i] = \gamma \mathbb{E}[d_i] h_i
\end{equation*}

Since $d_i$ is a Bernoulli random variable that takes value 0 with probability $p_{\text{drop}}$ and 1 with probability $1-p_{\text{drop}}$:

\begin{equation*}
\mathbb{E}[d_i] = 0 \cdot p_{\text{drop}} + 1 \cdot (1 - p_{\text{drop}}) = 1 - p_{\text{drop}}
\end{equation*}

Therefore:

\begin{equation*}
\mathbb{E}[\mathbf{h}_{\text{drop}}]_i = \gamma (1 - p_{\text{drop}}) h_i
\end{equation*}

For this to equal $h_i$ (as required by the constraint):

\begin{align*}
\gamma (1 - p_{\text{drop}}) h_i &= h_i \\
\gamma (1 - p_{\text{drop}}) &= 1 \\
\gamma &= \frac{1}{1 - p_{\text{drop}}}
\end{align*}

For example, if $p_{\text{drop}} = 0.5$ (dropping half the units), then $\gamma = \frac{1}{0.5} = 2$, which compensates by doubling the remaining activations so that the expected output magnitude stays constant.

\subsubsection*{(b)(ii) When to Apply Dropout (2 points)}

\textbf{Question:} Why should dropout be applied during training? Why should dropout NOT be applied during evaluation?

\textbf{Answer:}

\textbf{During training}, dropout should be applied because it acts as a powerful regularization technique that prevents overfitting. By randomly dropping units in each minibatch, dropout forces the network to learn robust and redundant representations that don't rely on the presence of specific neurons. This prevents co-adaptation of neurons (where neurons become overly dependent on each other) and encourages each neuron to learn independently useful features. The network essentially learns to work well even when parts of it are missing, which improves generalization to unseen data.

\textbf{During evaluation}, dropout should NOT be applied because we want deterministic, optimal predictions using all available learned features. Applying dropout at test time would introduce unnecessary randomness and reduce model performance by discarding useful information. Instead, we use all neurons with their learned weights, and the scaling factor $\gamma = \frac{1}{1-p_{\text{drop}}}$ applied during training ensures that the expected activations during training match the actual activations at test time (when all units are active). This principle is sometimes called "inverted dropout" and ensures smooth transition from training to evaluation without requiring weight scaling at test time.

\section*{3. Dependency Parsing}

\subsection*{(e) Neural Dependency Parsing}

\subsubsection*{(e)(i) Derivative of ReLU Layer with respect to Input (20 points)}

\textbf{Question:} Compute the derivative of $\mathbf{h} = \text{ReLU}(\mathbf{x}\mathbf{W} + \mathbf{b}_1)$ with respect to $\mathbf{x}$. For simplicity, show the derivative $\frac{\partial h_i}{\partial x_j}$ for some index $i$ and $j$. Ignore the case where the derivative is not defined at 0.

\textbf{Answer:}

\begin{equation*}
\frac{\partial h_i}{\partial x_j} = \mathbf{1}\{z_i > 0\} \cdot W_{ji}
\end{equation*}

where $z_i = (\mathbf{x}\mathbf{W} + \mathbf{b}_1)_i$ is the $i$-th element of the pre-activation, and $\mathbf{1}\{z_i > 0\}$ is an indicator function that equals 1 if $z_i > 0$ and 0 otherwise.

\textbf{Derivation:}

Let $\mathbf{z} = \mathbf{x}\mathbf{W} + \mathbf{b}_1$, so $\mathbf{h} = \text{ReLU}(\mathbf{z})$.

By the chain rule:
\begin{equation*}
\frac{\partial h_i}{\partial x_j} = \frac{\partial h_i}{\partial z_i} \cdot \frac{\partial z_i}{\partial x_j}
\end{equation*}

\textbf{Step 1:} Compute $\frac{\partial h_i}{\partial z_i}$.

Since $h_i = \text{ReLU}(z_i) = \max(z_i, 0)$:
\begin{equation*}
\frac{\partial h_i}{\partial z_i} = \begin{cases}
1 & \text{if } z_i > 0 \\
0 & \text{if } z_i < 0
\end{cases} = \mathbf{1}\{z_i > 0\}
\end{equation*}

\textbf{Step 2:} Compute $\frac{\partial z_i}{\partial x_j}$.

We have $z_i = \sum_{k} x_k W_{ki} + b_{1i}$ (the $i$-th element of $\mathbf{x}\mathbf{W} + \mathbf{b}_1$).

Taking the derivative with respect to $x_j$:
\begin{equation*}
\frac{\partial z_i}{\partial x_j} = W_{ji}
\end{equation*}

\textbf{Step 3:} Combine using the chain rule.

\begin{equation*}
\frac{\partial h_i}{\partial x_j} = \mathbf{1}\{z_i > 0\} \cdot W_{ji}
\end{equation*}

This shows that the gradient flow from $h_i$ back to $x_j$ is proportional to the weight $W_{ji}$, but only when the ReLU was active (i.e., $z_i > 0$). When the ReLU unit is inactive ($z_i \leq 0$), no gradient flows back through that path.

\subsubsection*{(e)(ii) Derivative of Cross-Entropy Loss with respect to Logits (20 points)}

\textbf{Question:} Compute the partial derivative of $J(\theta) = \text{CE}(\mathbf{y}, \hat{\mathbf{y}})$ with respect to the $i$-th entry of $\mathbf{l}$, denoted as $l_i$. Specifically, compute $\frac{\partial \text{CE}(\mathbf{y}, \hat{\mathbf{y}})}{\partial l_i}$, assuming that $\mathbf{l} \in \mathbb{R}^3$, $\hat{\mathbf{y}} \in \mathbb{R}^3$, $\mathbf{y} \in \mathbb{R}^3$, and the true label is $c$ (i.e., $y_j = 1$ if $j = c$). 

Hint: Use the chain rule: $\frac{\partial J}{\partial \mathbf{l}} = \frac{\partial J}{\partial \hat{\mathbf{y}}} \cdot \frac{\partial \hat{\mathbf{y}}}{\partial \mathbf{l}}$.

\textbf{Answer:}

\begin{equation*}
\frac{\partial \text{CE}(\mathbf{y}, \hat{\mathbf{y}})}{\partial l_i} = \hat{y}_i - y_i
\end{equation*}

where $y_i = 1$ if $i = c$ (the true class) and $y_i = 0$ otherwise.

\textbf{Derivation:}

The cross-entropy loss is:
\begin{equation*}
\text{CE}(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{3} y_j \log \hat{y}_j
\end{equation*}

We have $\hat{\mathbf{y}} = \text{softmax}(\mathbf{l})$, where:
\begin{equation*}
\hat{y}_k = \frac{\exp(l_k)}{\sum_{j=1}^{3} \exp(l_j)}
\end{equation*}

Using the chain rule:
\begin{equation*}
\frac{\partial \text{CE}}{\partial l_i} = \sum_{k=1}^{3} \frac{\partial \text{CE}}{\partial \hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial l_i}
\end{equation*}

\textbf{Step 1:} Compute $\frac{\partial \text{CE}}{\partial \hat{y}_k}$.

\begin{equation*}
\frac{\partial \text{CE}}{\partial \hat{y}_k} = \frac{\partial}{\partial \hat{y}_k}\left(-\sum_{j=1}^{3} y_j \log \hat{y}_j\right) = -\frac{y_k}{\hat{y}_k}
\end{equation*}

\textbf{Step 2:} Compute $\frac{\partial \hat{y}_k}{\partial l_i}$.

For the softmax function, we need to consider two cases:

\textbf{Case 2a:} When $k = i$:
\begin{align*}
\frac{\partial \hat{y}_i}{\partial l_i} &= \frac{\partial}{\partial l_i}\left(\frac{\exp(l_i)}{\sum_j \exp(l_j)}\right) \\
&= \frac{\exp(l_i) \cdot \sum_j \exp(l_j) - \exp(l_i) \cdot \exp(l_i)}{(\sum_j \exp(l_j))^2} \\
&= \frac{\exp(l_i)}{\sum_j \exp(l_j)} \cdot \frac{\sum_j \exp(l_j) - \exp(l_i)}{\sum_j \exp(l_j)} \\
&= \hat{y}_i(1 - \hat{y}_i)
\end{align*}

\textbf{Case 2b:} When $k \neq i$:
\begin{align*}
\frac{\partial \hat{y}_k}{\partial l_i} &= \frac{\partial}{\partial l_i}\left(\frac{\exp(l_k)}{\sum_j \exp(l_j)}\right) \\
&= \frac{0 \cdot \sum_j \exp(l_j) - \exp(l_k) \cdot \exp(l_i)}{(\sum_j \exp(l_j))^2} \\
&= -\frac{\exp(l_k)}{\sum_j \exp(l_j)} \cdot \frac{\exp(l_i)}{\sum_j \exp(l_j)} \\
&= -\hat{y}_k \hat{y}_i
\end{align*}

\textbf{Step 3:} Combine using the chain rule.

\begin{align*}
\frac{\partial \text{CE}}{\partial l_i} &= \sum_{k=1}^{3} \frac{\partial \text{CE}}{\partial \hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial l_i} \\
&= \frac{\partial \text{CE}}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial l_i} + \sum_{k \neq i} \frac{\partial \text{CE}}{\partial \hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial l_i} \\
&= \left(-\frac{y_i}{\hat{y}_i}\right) \cdot \hat{y}_i(1 - \hat{y}_i) + \sum_{k \neq i} \left(-\frac{y_k}{\hat{y}_k}\right) \cdot (-\hat{y}_k \hat{y}_i) \\
&= -y_i(1 - \hat{y}_i) + \sum_{k \neq i} y_k \hat{y}_i \\
&= -y_i + y_i \hat{y}_i + \hat{y}_i \sum_{k \neq i} y_k \\
&= -y_i + \hat{y}_i\left(y_i + \sum_{k \neq i} y_k\right) \\
&= -y_i + \hat{y}_i \sum_{k=1}^{3} y_k
\end{align*}

Since $\mathbf{y}$ is a one-hot vector (i.e., $\sum_{k=1}^{3} y_k = 1$):
\begin{equation*}
\frac{\partial \text{CE}}{\partial l_i} = -y_i + \hat{y}_i \cdot 1 = \hat{y}_i - y_i
\end{equation*}

This elegant result shows that the gradient of cross-entropy loss with respect to the logits is simply the difference between the predicted probabilities and the true labels. When $i = c$ (the true class), the gradient is $\hat{y}_c - 1$, which is negative and pushes the logit $l_c$ higher. When $i \neq c$, the gradient is $\hat{y}_i - 0 = \hat{y}_i$, which is positive and pushes the logit $l_i$ lower.

\end{document}
